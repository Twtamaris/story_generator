{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = [\n",
    "\"\"\"\"The Forgotten Map\": In a dusty old library, a young researcher discovers a mysterious map hidden within the pages of an ancient book. The map seems to lead to a long-lost kingdom filled with treasures and secrets. As the researcher delves deeper into deciphering the map's clues, they uncover unexpected challenges and adversaries determined to reach the treasure first.\"\"\",\n",
    "\n",
    "\"\"\"\"Whispers in the Woods\": In a secluded forest, strange whispers are heard by anyone who dares to enter. A curious teenager, known for their bravery, decides to investigate. As they venture deeper into the woods, they encounter enigmatic creatures and learn that the whispers hold the key to a forgotten prophecy that could change the fate of their world.\"\"\",\n",
    "\n",
    "\"\"\"\"The Last Beacon\": In a world plunged into darkness after an ancient catastrophe, a young engineer discovers an ancient lighthouse said to hold the power to restore light to the world. With the help of a group of unlikely allies, the engineer embarks on a perilous journey across treacherous lands, facing formidable challenges and adversaries who seek to keep the world in eternal darkness.\"\"\",\n",
    "\n",
    "\"\"\"\"Echoes of Time\": A gifted physicist invents a time-traveling device capable of sending messages to the past. However, the messages sent cause unforeseen ripples through time, altering events in unexpected ways. As the fabric of reality begins to unravel, the physicist races against time to correct the changes before irreparable damage occurs.\"\"\",\n",
    "\n",
    "\"\"\"\"The Dream Catcher\": In a town plagued by a series of haunting nightmares, a young artist discovers they have the ability to enter dreams and alter their course. Tasked with unraveling the mystery behind these nightmares, they navigate a surreal dream world, facing manifestations of people's deepest fears and secrets, all while trying to prevent a looming catastrophe that threatens to merge the dream realm with reality.\"\"\",\n",
    "    \n",
    "    # Add more stories here\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this checks the longest story with most number of words\n",
    "max_len = max(len(story.split()) for story in stories)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000  # Choose an appropriate vocabulary size\n",
    "embedding_dim = 100  # Embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(stories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1,\n",
       "  22,\n",
       "  11,\n",
       "  5,\n",
       "  2,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  2,\n",
       "  12,\n",
       "  23,\n",
       "  13,\n",
       "  2,\n",
       "  45,\n",
       "  11,\n",
       "  46,\n",
       "  47,\n",
       "  1,\n",
       "  48,\n",
       "  4,\n",
       "  14,\n",
       "  15,\n",
       "  49,\n",
       "  1,\n",
       "  11,\n",
       "  50,\n",
       "  3,\n",
       "  51,\n",
       "  3,\n",
       "  2,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  9,\n",
       "  56,\n",
       "  6,\n",
       "  24,\n",
       "  16,\n",
       "  1,\n",
       "  23,\n",
       "  57,\n",
       "  25,\n",
       "  17,\n",
       "  58,\n",
       "  1,\n",
       "  59,\n",
       "  60,\n",
       "  7,\n",
       "  61,\n",
       "  26,\n",
       "  27,\n",
       "  6,\n",
       "  28,\n",
       "  62,\n",
       "  3,\n",
       "  63,\n",
       "  1,\n",
       "  64,\n",
       "  65],\n",
       " [18,\n",
       "  5,\n",
       "  1,\n",
       "  29,\n",
       "  5,\n",
       "  2,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  18,\n",
       "  69,\n",
       "  70,\n",
       "  30,\n",
       "  71,\n",
       "  31,\n",
       "  72,\n",
       "  3,\n",
       "  32,\n",
       "  2,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  19,\n",
       "  77,\n",
       "  78,\n",
       "  3,\n",
       "  79,\n",
       "  16,\n",
       "  7,\n",
       "  80,\n",
       "  25,\n",
       "  17,\n",
       "  1,\n",
       "  29,\n",
       "  7,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  6,\n",
       "  84,\n",
       "  20,\n",
       "  1,\n",
       "  18,\n",
       "  33,\n",
       "  1,\n",
       "  85,\n",
       "  3,\n",
       "  2,\n",
       "  22,\n",
       "  86,\n",
       "  20,\n",
       "  87,\n",
       "  88,\n",
       "  1,\n",
       "  89,\n",
       "  4,\n",
       "  19,\n",
       "  8],\n",
       " [1,\n",
       "  90,\n",
       "  91,\n",
       "  5,\n",
       "  2,\n",
       "  8,\n",
       "  92,\n",
       "  17,\n",
       "  34,\n",
       "  93,\n",
       "  14,\n",
       "  15,\n",
       "  35,\n",
       "  2,\n",
       "  12,\n",
       "  36,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  94,\n",
       "  95,\n",
       "  3,\n",
       "  33,\n",
       "  1,\n",
       "  96,\n",
       "  3,\n",
       "  97,\n",
       "  98,\n",
       "  3,\n",
       "  1,\n",
       "  8,\n",
       "  9,\n",
       "  1,\n",
       "  99,\n",
       "  4,\n",
       "  2,\n",
       "  100,\n",
       "  4,\n",
       "  101,\n",
       "  102,\n",
       "  1,\n",
       "  36,\n",
       "  103,\n",
       "  104,\n",
       "  2,\n",
       "  105,\n",
       "  106,\n",
       "  107,\n",
       "  108,\n",
       "  109,\n",
       "  37,\n",
       "  110,\n",
       "  27,\n",
       "  6,\n",
       "  28,\n",
       "  31,\n",
       "  111,\n",
       "  3,\n",
       "  112,\n",
       "  1,\n",
       "  8,\n",
       "  5,\n",
       "  113,\n",
       "  34],\n",
       " [114,\n",
       "  4,\n",
       "  10,\n",
       "  2,\n",
       "  115,\n",
       "  38,\n",
       "  116,\n",
       "  2,\n",
       "  10,\n",
       "  117,\n",
       "  118,\n",
       "  119,\n",
       "  4,\n",
       "  120,\n",
       "  39,\n",
       "  3,\n",
       "  1,\n",
       "  121,\n",
       "  122,\n",
       "  1,\n",
       "  39,\n",
       "  123,\n",
       "  124,\n",
       "  125,\n",
       "  126,\n",
       "  127,\n",
       "  10,\n",
       "  128,\n",
       "  129,\n",
       "  5,\n",
       "  26,\n",
       "  130,\n",
       "  16,\n",
       "  1,\n",
       "  131,\n",
       "  4,\n",
       "  40,\n",
       "  132,\n",
       "  3,\n",
       "  133,\n",
       "  1,\n",
       "  38,\n",
       "  134,\n",
       "  135,\n",
       "  10,\n",
       "  3,\n",
       "  136,\n",
       "  1,\n",
       "  137,\n",
       "  138,\n",
       "  139,\n",
       "  140,\n",
       "  141],\n",
       " [1,\n",
       "  21,\n",
       "  142,\n",
       "  5,\n",
       "  2,\n",
       "  143,\n",
       "  144,\n",
       "  30,\n",
       "  2,\n",
       "  145,\n",
       "  4,\n",
       "  146,\n",
       "  41,\n",
       "  2,\n",
       "  12,\n",
       "  147,\n",
       "  13,\n",
       "  7,\n",
       "  148,\n",
       "  1,\n",
       "  149,\n",
       "  3,\n",
       "  32,\n",
       "  150,\n",
       "  6,\n",
       "  151,\n",
       "  19,\n",
       "  152,\n",
       "  153,\n",
       "  9,\n",
       "  154,\n",
       "  1,\n",
       "  155,\n",
       "  156,\n",
       "  157,\n",
       "  41,\n",
       "  7,\n",
       "  158,\n",
       "  2,\n",
       "  159,\n",
       "  21,\n",
       "  8,\n",
       "  37,\n",
       "  160,\n",
       "  4,\n",
       "  161,\n",
       "  162,\n",
       "  163,\n",
       "  6,\n",
       "  24,\n",
       "  164,\n",
       "  165,\n",
       "  166,\n",
       "  3,\n",
       "  167,\n",
       "  2,\n",
       "  168,\n",
       "  35,\n",
       "  20,\n",
       "  169,\n",
       "  3,\n",
       "  170,\n",
       "  1,\n",
       "  21,\n",
       "  171,\n",
       "  9,\n",
       "  40]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(stories)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [\n",
    "#     [1, 2, 3, 4, 1, 5],\n",
    "#     [1, 6, 7, 8, 9],\n",
    "#     [10, 11, 12, 4, 1, 13]\n",
    "# ]\n",
    "## converts this code to this using\n",
    "# padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "# [\n",
    "#     [1, 2, 3, 4, 1, 5],    # Unchanged - already at max_len\n",
    "#     [1, 6, 7, 8, 9, 0],   # Padded with zeros at the end\n",
    "#     [10, 11, 12, 4, 1, 13]  # Unchanged - already at max_len\n",
    "# ]\n",
    "# # \n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 67) for input KerasTensor(type_spec=TensorSpec(shape=(None, 67), dtype=tf.float32, name='embedding_9_input'), name='embedding_9_input', description=\"created by layer 'embedding_9_input'\"), but it was called on an input with incompatible shape (None, 10000).\n"
     ]
    }
   ],
   "source": [
    "# Building the generator model\n",
    "generator = Sequential()\n",
    "generator.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len))\n",
    "generator.add(Bidirectional(LSTM(128)))\n",
    "# activation='softmax' is used because the generator's output should represent \n",
    "# probabilities for each element in vocab_size in the vocabulary.\n",
    "generator.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "# Building the discriminator model\n",
    "discriminator = Sequential()\n",
    "discriminator.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len))\n",
    "discriminator.add(Bidirectional(LSTM(128)))\n",
    "discriminator.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5))\n",
    "\n",
    "# Combining generator and discriminator as part of GAN\n",
    "discriminator.trainable = False\n",
    "gan_input = generator.input\n",
    "gan_output = discriminator(generator(gan_input))\n",
    "gan = Model(gan_input, gan_output)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 53ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "3\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "4\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "5\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "6\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "7\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "8\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "9\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "10\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "11\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "12\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "13\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "14\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "15\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "16\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "17\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "18\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "19\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "20\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "21\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "22\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "23\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "24\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "25\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "26\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "27\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "28\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "29\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "30\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "31\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "32\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "33\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "34\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "35\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "36\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "37\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "38\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "39\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "40\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "41\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "42\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "43\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "44\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "45\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "46\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "47\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "48\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "49\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "50\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "51\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "52\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "53\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "54\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "55\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "56\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "57\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "58\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "59\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "60\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "61\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "62\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "63\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "64\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "65\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "66\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "67\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "68\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "69\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "70\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "71\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "72\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "73\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "74\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "75\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "76\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "77\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "78\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "79\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "80\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "81\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "82\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "83\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "84\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "85\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "86\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "87\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "88\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "89\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "90\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "91\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "92\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "93\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "94\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "95\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "96\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "97\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "98\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "# Training the GAN\n",
    "epochs = 100 # Choose an appropriate number of epochs\n",
    "batch_size = 32  # Define batch size\n",
    "for epoch in range(epochs):\n",
    "    noise = np.random.randint(0, vocab_size, size=(batch_size, max_len))\n",
    "    generated_stories = generator.predict(noise)\n",
    "    real_stories = padded_sequences[np.random.randint(0, len(padded_sequences), size=batch_size)]\n",
    "    print(epoch)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 67)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_stories.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 67) for input KerasTensor(type_spec=TensorSpec(shape=(None, 67), dtype=tf.float32, name='embedding_7_input'), name='embedding_7_input', description=\"created by layer 'embedding_7_input'\"), but it was called on an input with incompatible shape (None, 10000).\n",
      "1/1 [==============================] - 1s 935ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 67 and the array at index 1 has size 10000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 40>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m generated_stories \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mpredict(noise)\n\u001b[0;32m     43\u001b[0m real_stories \u001b[38;5;241m=\u001b[39m padded_sequences[np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(padded_sequences), size\u001b[38;5;241m=\u001b[39mbatch_size)]\n\u001b[1;32m---> 44\u001b[0m x_combined \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mreal_stories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerated_stories\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m y_combined \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([np\u001b[38;5;241m.\u001b[39mones((batch_size, \u001b[38;5;241m1\u001b[39m)), np\u001b[38;5;241m.\u001b[39mzeros((batch_size, \u001b[38;5;241m1\u001b[39m))])\n\u001b[0;32m     46\u001b[0m d_loss \u001b[38;5;241m=\u001b[39m discriminator\u001b[38;5;241m.\u001b[39mtrain_on_batch(x_combined, y_combined)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 67 and the array at index 1 has size 10000"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
